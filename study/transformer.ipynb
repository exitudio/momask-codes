{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Change to Uplow update these following\n",
    "- /home/epinyoan/git/momask-codes/models/mask_transformer/transformer_trainer.py\n",
    "- /home/epinyoan/git/momask-codes/models/vq/model.py\n",
    "- /home/epinyoan/git/momask-codes/models/mask_transformer/transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2M-GPT Transformer change these:\n",
    "from 2024-01-19-20-40-27_2_GPT_randId0-.5_weightBySample_t2mgptTrans_logEvry50_n_layers4\n",
    "- /models/mask_transformer/tools.py (only one line: just don't compute loss here)\n",
    "- /models/mask_transformer/transformer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epinyoan/miniconda3/envs/momask/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from models.mask_transformer.transformer import MaskTransformer\n",
    "from models.mask_transformer.transformer_trainer import MaskTransformerTrainer\n",
    "from models.vq.model import RVQVAE\n",
    "\n",
    "from options.train_option import TrainT2MOptions\n",
    "\n",
    "from utils.plot_script import plot_3d_motion\n",
    "from utils.motion_process import recover_from_ric\n",
    "from utils.get_opt import get_opt\n",
    "from utils.fixseed import fixseed\n",
    "from utils.paramUtil import t2m_kinematic_chain, kit_kinematic_chain\n",
    "\n",
    "from data.t2m_dataset import Text2MotionDataset\n",
    "from motion_loaders.dataset_motion_loader import get_dataset_motion_loader\n",
    "from models.t2m_eval_wrapper import EvaluatorModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batch_size: 64\n",
      "checkpoints_dir: ./log/t2m\n",
      "cond_drop_prob: 0.1\n",
      "dataset_name: t2m\n",
      "dropout: 0.2\n",
      "eval_every_e: 10\n",
      "ff_size: 1024\n",
      "force_mask: False\n",
      "gamma: 0.1\n",
      "gpu_id: -1\n",
      "gumbel_sample: False\n",
      "is_continue: False\n",
      "is_train: True\n",
      "latent_dim: 384\n",
      "log_every: 50\n",
      "lr: 0.0002\n",
      "max_epoch: 500\n",
      "max_motion_length: 196\n",
      "milestones: [50000]\n",
      "n_heads: 6\n",
      "n_layers: 8\n",
      "name: 2024-01-21-19-57-56_t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns\n",
      "save_latest: 500\n",
      "seed: 3407\n",
      "share_weight: False\n",
      "unit_length: 4\n",
      "vq_name: rvq_nq1_dc512_nc512\n",
      "warm_up_iter: 2000\n",
      "-------------- End ----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-01-21-19-57-56_t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from options.train_option import TrainT2MOptions\n",
    "parser = TrainT2MOptions()\n",
    "opt = parser.parse(is_mock=True)\n",
    "opt.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.data_root = './dataset/HumanML3D'\n",
    "opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "opt.joints_num = 22\n",
    "opt.max_motion_len = 55\n",
    "dim_pose = 263\n",
    "radius = 4\n",
    "fps = 20\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "dataset_opt_path = './checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/epinyoan/git/momask-codes/checkpoints/t2m/rvq_nq6_dc512_nc512_noshare_qdp0.2/opt.txt\n",
      "Loading VQ Model rvq_nq6_dc512_nc512_noshare_qdp0.2\n"
     ]
    }
   ],
   "source": [
    "opt.vq_name = 'rvq_nq6_dc512_nc512_noshare_qdp0.2'\n",
    "vq_opt = get_opt(f'/home/epinyoan/git/momask-codes/checkpoints/t2m/{opt.vq_name}/opt.txt', 'cuda')\n",
    "vq_model = RVQVAE(vq_opt,\n",
    "                dim_pose,\n",
    "                vq_opt.nb_code,\n",
    "                vq_opt.code_dim,\n",
    "                vq_opt.output_emb_width,\n",
    "                vq_opt.down_t,\n",
    "                vq_opt.stride_t,\n",
    "                vq_opt.width,\n",
    "                vq_opt.depth,\n",
    "                vq_opt.dilation_growth_rate,\n",
    "                vq_opt.vq_act,\n",
    "                vq_opt.vq_norm)\n",
    "ckpt = torch.load(pjoin(vq_opt.checkpoints_dir, vq_opt.dataset_name, vq_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                        map_location='cpu')\n",
    "model_key = 'vq_model' if 'vq_model' in ckpt else 'net'\n",
    "vq_model.load_state_dict(ckpt[model_key])\n",
    "print(f'Loading VQ Model {opt.vq_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n",
      "Loading CLIP...\n"
     ]
    }
   ],
   "source": [
    "opt.log_dir = pjoin('./log/t2m/', opt.dataset_name, opt.name)\n",
    "opt.device = 'cuda' #torch.device(\"cpu\" if opt.gpu_id == -1 else \"cuda:\" + str(opt.gpu_id))\n",
    "opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "opt.num_tokens = vq_opt.nb_code\n",
    "clip_version = 'ViT-B/32'\n",
    "t2m_transformer = MaskTransformer(code_dim=vq_opt.code_dim,\n",
    "                                      cond_mode='text',\n",
    "                                      latent_dim=opt.latent_dim,\n",
    "                                      ff_size=opt.ff_size,\n",
    "                                      num_layers=opt.n_layers,\n",
    "                                      num_heads=opt.n_heads,\n",
    "                                      dropout=opt.dropout,\n",
    "                                      clip_dim=512,\n",
    "                                      cond_drop_prob=opt.cond_drop_prob,\n",
    "                                      clip_version=clip_version,\n",
    "                                      opt=opt)\n",
    "# t2m_transformer.load_and_freeze_token_emb(vq_model.quantizer_upper.codebooks[0], vq_model.quantizer_lower.codebooks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = '/home/epinyoan/git/momask-codes/log/t2m/t2m/2024-01-18-23-27-31_2_GPT_randId0-.5_weightBySample/model/latest.tar'\n",
    "model_dir = '/home/epinyoan/git/momask-codes/log/t2m/t2m/2024-01-21-09-10-30_3_GPT_randId0-.5_weightBySample_FIXED_TRAIN_END_TOKEN_b256/model/latest.tar'\n",
    "checkpoint = torch.load(model_dir, map_location='cuda')\n",
    "missing_keys, unexpected_keys = t2m_transformer.load_state_dict(checkpoint['t2m_transformer'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 222/23384 [00:00<00:10, 2212.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:12<00:00, 1871.88it/s]\n",
      "100%|██████████| 1460/1460 [00:00<00:00, 1862.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading dataset t2m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 1890.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Ground Truth Dataset Loading Completed!!!\n",
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "mean = np.load(pjoin('./checkpoints/', opt.dataset_name, opt.vq_name, 'meta', 'mean.npy'))\n",
    "std = np.load(pjoin('./checkpoints/', opt.dataset_name, opt.vq_name, 'meta', 'std.npy'))\n",
    "\n",
    "train_split_file = pjoin(opt.data_root, 'train.txt')\n",
    "val_split_file = pjoin(opt.data_root, 'val.txt')\n",
    "\n",
    "train_dataset = Text2MotionDataset(opt, mean, std, train_split_file)\n",
    "val_dataset = Text2MotionDataset(opt, mean, std, val_split_file)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=opt.batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "eval_val_loader, _ = get_dataset_motion_loader(dataset_opt_path, 32, 'val', device=opt.device)\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.is_train = False # to turn of tensorboard\n",
    "trainer = MaskTransformerTrainer(opt, t2m_transformer, vq_model)\n",
    "# trainer.resume(model_dir = '/home/epinyoan/git/momask-codes/log/t2m/t2m/2024-01-13-22-25-36_1_uplow_vq512emb_mileStone100k_800ep/model/latest.tar')\n",
    "# vq_model.moment = {'mean': torch.from_numpy(train_loader.dataset.mean).cuda().float(), \n",
    "#                 'std': torch.from_numpy(train_loader.dataset.std).cuda().float()}\n",
    "t2m_transformer.to(opt.device)\n",
    "vq_model.to(opt.device)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ids: torch.Size([64, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.8699, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " torch.Size([64, 50]),\n",
       " 0.4661746621131897)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = next(iter(train_loader))\n",
    "conds, motion, m_lens = batch_data\n",
    "motion = motion.detach().float().to(opt.device)\n",
    "m_lens = m_lens.detach().long().to(opt.device)\n",
    "\n",
    "# (b, n, q)\n",
    "code_idx, _ = vq_model.encode(motion)\n",
    "m_lens = m_lens // 4\n",
    "\n",
    "conds = conds.to(opt.device).float() if torch.is_tensor(conds) else conds\n",
    "\n",
    "_loss, _pred_ids, _acc = t2m_transformer(code_idx[..., 0], conds, m_lens)\n",
    "_loss, _pred_ids.shape, _acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  2,  3,  4,  5,  6],\n",
       "         [ 1,  2,  3,  4, -1, -1],\n",
       "         [ 1, -1, -1, -1, -1, -1],\n",
       "         [ 0, -1, -1, -1, -1, -1]], device='cuda:0'),\n",
       " tensor([7, 4, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \"pad_when_end\"\n",
    "xs = torch.tensor([[1,2,3,4,5,6],\n",
    "                   [1,2,3,4,512,512],\n",
    "                   [1,512,3,4,5,512],\n",
    "                   [512,2,3,4,5,6],], device='cuda')\n",
    "t2m_transformer.pad_when_end(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token = next(iter(eval_val_loader))\n",
    "m_length = m_length.cuda()\n",
    "time_steps = 18\n",
    "cond_scale = 4\n",
    "mids = t2m_transformer.generate(clip_text, m_length//4, time_steps, cond_scale, temperature=1)\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids.unsqueeze_(-1)\n",
    "pred_motions = vq_model.forward_decoder(mids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 49, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Ep 0 :, FID. 0.1432, Diversity Real. 9.9017, Diversity. 9.6550, R_precision_real. [0.51795213 0.69481383 0.79654255], R_precision. [0.50864362 0.6974734  0.79654255], matching_score_real. 2.9001340003723795, matching_score_pred. 2.9835535262493376\n",
      "./Test/FID 0.1432133627930483 0\n",
      "./Test/Diversity 9.655023 0\n",
      "./Test/top1 0.5086436170212766 0\n",
      "./Test/top2 0.6974734042553191 0\n",
      "./Test/top3 0.7965425531914894 0\n",
      "./Test/matching_score 2.9835535262493376 0\n",
      "--> --> \t FID Improved from 100.00000 to 0.14321 !!!\n",
      "--> --> \t matching_score Improved from 100.00000 to 2.98355 !!!\n",
      "--> --> \t Diversity Improved from 100.00000 to 9.65502 !!!\n",
      "--> --> \t Top1 Improved from 0.0000 to 0.5086 !!!\n",
      "--> --> \t Top2 Improved from 0.0000 to 0.6975 !!!\n",
      "--> --> \t Top3 Improved from 0.0000 to 0.7965 !!!\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_t2m import evaluation_mask_transformer\n",
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "best_fid, best_div, best_top1, best_top2, best_top3, best_matching, writer = evaluation_mask_transformer(\n",
    "        'opt.save_root', eval_val_loader, t2m_transformer, vq_model, logger, 0,\n",
    "        best_fid=100, best_div=100,\n",
    "        best_top1=0, best_top2=0, best_top3=0,\n",
    "        best_matching=100, eval_wrapper=eval_wrapper,\n",
    "        plot_func=lambda x:x, save_ckpt=False, save_anim=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
